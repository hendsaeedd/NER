{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dde6435",
   "metadata": {},
   "source": [
    "# import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7885e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#to open csv file\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#sentences & words tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#regular expression \n",
    "import re\n",
    "#for stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "#n-grams\n",
    "from nltk.util import ngrams\n",
    "#Lemmatization & Stemming\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f707a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6451e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58a1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b250c",
   "metadata": {},
   "source": [
    "# csv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8d5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r'C:\\Users\\DELL\\OneDrive\\code\\py\\nlp\\NERdataset\\NER-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c72b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(dataset_path, encoding=\"ISO-8859-1\")\n",
    "dataset.head()\n",
    "#UnicodeDecodeError: 'utf-8' codec can't decode byte 0x85 in position 25560: invalid start byte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1debf73",
   "metadata": {},
   "source": [
    "# number of sentences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bdad3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 460\n",
      "Number of words: 10075\n"
     ]
    }
   ],
   "source": [
    "#number of sentences \n",
    "sentences = dataset['Sentence'].count()\n",
    "\n",
    "#number of words\n",
    "words = dataset['Word'].count()\n",
    "\n",
    "print(f\"Number of sentences: {sentences}\")\n",
    "print(f\"Number of words: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8777042f",
   "metadata": {},
   "source": [
    "# number of stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47255b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#punctuation = set(string.punctuation)\n",
    "punctuation = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e94a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stopwords = 0\n",
    "num_punctuation = 0\n",
    "\n",
    "for i in dataset['Word']:\n",
    "    if i in stop_words:\n",
    "        num_stopwords += 1\n",
    "    elif i in string.punctuation:\n",
    "        num_punctuation += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21729542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 3202\n",
      "Number of punctuation: 800\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stopwords:\", num_stopwords)\n",
    "# print(stop_words)\n",
    "print(\"Number of punctuation:\", num_punctuation)\n",
    "# print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938d381",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0cd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 3 \n",
    "# ngrams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "329e6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dataset['Word']) - n + 1):\n",
    "#     ngram = ' '.join(dataset['Word'][i:i+n])\n",
    "#     ngrams.append(ngram)\n",
    "\n",
    "# for ngram in ngrams:\n",
    "#     print([ngram])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5aa259",
   "metadata": {},
   "source": [
    "# preprocessing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff51b3",
   "metadata": {},
   "source": [
    "## Lowercasing:\n",
    "#### Convert all text to lowercase. This ensures that the model doesn't treat \"Apple\" and \"apple\" as different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab7cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a03a9482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            thousands\n",
       "1                   of\n",
       "2        demonstrators\n",
       "3                 have\n",
       "4              marched\n",
       "             ...      \n",
       "10070             more\n",
       "10071             than\n",
       "10072            4,500\n",
       "10073            years\n",
       "10074                .\n",
       "Name: Word, Length: 10075, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Word'] = dataset['Word'].str.lower()\n",
    "dataset['Word']\n",
    "\n",
    "# clean_dataset.extend(dataset['Word'].tolist())\n",
    "# print(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0843903",
   "metadata": {},
   "source": [
    "## Tokenization:\n",
    "#### Tokenize your text into words or subword units. This breaks down the text into individual units, making it easier for the model to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe38f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thousands</td>\n",
       "      <td>[thousands]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>[of]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>[demonstrators]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>[have]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>[marched]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10070</th>\n",
       "      <td>more</td>\n",
       "      <td>[more]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10071</th>\n",
       "      <td>than</td>\n",
       "      <td>[than]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10072</th>\n",
       "      <td>4,500</td>\n",
       "      <td>[4,500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10073</th>\n",
       "      <td>years</td>\n",
       "      <td>[years]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10074</th>\n",
       "      <td>.</td>\n",
       "      <td>[.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10075 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word        Tokenized\n",
       "0          thousands      [thousands]\n",
       "1                 of             [of]\n",
       "2      demonstrators  [demonstrators]\n",
       "3               have           [have]\n",
       "4            marched        [marched]\n",
       "...              ...              ...\n",
       "10070           more           [more]\n",
       "10071           than           [than]\n",
       "10072          4,500          [4,500]\n",
       "10073          years          [years]\n",
       "10074              .              [.]\n",
       "\n",
       "[10075 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(word):\n",
    "    return word_tokenize(word)\n",
    "\n",
    "dataset['Tokenized'] = dataset['Word'].apply(tokenize_text)\n",
    "dataset = dataset[dataset['Word'] != '']\n",
    "dataset[['Word', 'Tokenized']]\n",
    "# clean_dataset = dataset['Tokenized'].tolist()\n",
    "# print(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5953cd2",
   "metadata": {},
   "source": [
    "## Removing Stopwords:\n",
    "#### Remove common stopwords as they usually do not carry much information for NER tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88fe2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#punctuation = set(string.punctuation)\n",
    "punctuation = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c63e2f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            thousands\n",
       "1                     \n",
       "2        demonstrators\n",
       "3                     \n",
       "4              marched\n",
       "             ...      \n",
       "10070                 \n",
       "10071                 \n",
       "10072            4,500\n",
       "10073            years\n",
       "10074                .\n",
       "Name: Word, Length: 10075, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords and replace with an empty string\n",
    "def remove_stopwords(word):\n",
    "\n",
    "    return word if word not in stop_words else ''\n",
    "\n",
    "dataset['Word'] = dataset['Word'].apply(remove_stopwords)\n",
    "dataset['Word']\n",
    "# dataset = dataset[dataset['Word'] != '']\n",
    "# clean_dataset = dataset['Word'].tolist()\n",
    "# print(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1d9f6",
   "metadata": {},
   "source": [
    "## Removing Punctuation:\n",
    "#### Depending on your task, you might want to remove or replace punctuation. In NER, punctuation may not contribute much to identifying entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76708dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            thousands\n",
       "1                     \n",
       "2        demonstrators\n",
       "3                     \n",
       "4              marched\n",
       "             ...      \n",
       "10070                 \n",
       "10071                 \n",
       "10072            4,500\n",
       "10073            years\n",
       "10074                 \n",
       "Name: Word, Length: 10075, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(word):\n",
    "\n",
    "    return word if word not in punctuation else ''\n",
    "\n",
    "dataset['Word'] = dataset['Word'].apply(remove_punctuation)\n",
    "dataset['Word']\n",
    "# dataset = dataset[dataset['Word'] != '']\n",
    "# clean_dataset = dataset['Word'].tolist()\n",
    "# print(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd1271",
   "metadata": {},
   "source": [
    "## Lemmatization or Stemming:\n",
    "#### Reduce words to their base or root form. This helps in reducing dimensionality and treating different forms of a word as the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b90ad9",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e77e62e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thousands</td>\n",
       "      <td>thousand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>demonstrator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>marched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10070</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10071</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10072</th>\n",
       "      <td>4,500</td>\n",
       "      <td>4,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10073</th>\n",
       "      <td>years</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10074</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10075 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word    Lemmatized\n",
       "0          thousands      thousand\n",
       "1                                 \n",
       "2      demonstrators  demonstrator\n",
       "3                                 \n",
       "4            marched       marched\n",
       "...              ...           ...\n",
       "10070                             \n",
       "10071                             \n",
       "10072          4,500         4,500\n",
       "10073          years          year\n",
       "10074                             \n",
       "\n",
       "[10075 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_word(word):\n",
    "    if word is not None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return lemmatizer.lemmatize(word)\n",
    "    \n",
    "dataset['Lemmatized'] = dataset['Word'].apply(lemmatize_word)\n",
    "dataset[['Word','Lemmatized']]\n",
    "# dataset = dataset[dataset['Word'] != '']\n",
    "# clean_dataset = dataset['Lemmatized'].tolist()\n",
    "# print(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6885ab15",
   "metadata": {},
   "source": [
    "### preprocessing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1d4e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Apply lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove punctuation\n",
    "    punctuation = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79603845",
   "metadata": {},
   "source": [
    "# clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edb31b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_dataset']=dataset['Lemmatized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f8282",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a81674d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "10070    0\n",
       "10071    0\n",
       "10072    0\n",
       "10073    0\n",
       "10074    0\n",
       "Name: POS, Length: 10075, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset['POS'] = dataset.POS.map({'NNP': 1})\n",
    "def map_pos(v):\n",
    "    return 1 if v == 'NNP' else 0  #we will target COUNTRIES only\n",
    "\n",
    "dataset['POS'] = dataset['POS'].apply(map_pos)\n",
    "dataset['POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a44366b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thousand: 7\n",
      ": 4236\n",
      "demonstrator: 1\n",
      "marched: 2\n",
      "london: 8\n",
      "protest: 8\n",
      "war: 14\n",
      "iraq: 16\n",
      "demand: 3\n",
      "withdrawal: 1\n",
      "british: 14\n",
      "troop: 17\n",
      "country: 28\n",
      "family: 2\n",
      "soldier: 9\n",
      "killed: 29\n",
      "conflict: 3\n",
      "joined: 1\n",
      "protester: 3\n",
      "carried: 3\n",
      "banner: 1\n",
      "slogan: 2\n",
      "bush: 7\n",
      "number: 5\n",
      "one: 22\n",
      "terrorist: 4\n",
      "stop: 5\n",
      "bombing: 10\n",
      "house: 6\n",
      "parliament: 4\n",
      "rally: 1\n",
      "hyde: 1\n",
      "park: 1\n",
      "police: 22\n",
      "put: 6\n",
      "marcher: 1\n",
      "10,000: 1\n",
      "organizer: 3\n",
      "claimed: 4\n",
      "1,00,000: 1\n",
      "come: 5\n",
      "eve: 1\n",
      "annual: 6\n",
      "conference: 3\n",
      "britain: 3\n",
      "'s: 98\n",
      "ruling: 1\n",
      "labor: 4\n",
      "party: 9\n",
      "southern: 9\n",
      "english: 2\n",
      "seaside: 1\n",
      "resort: 1\n",
      "brighton: 1\n",
      "divided: 1\n",
      "participation: 1\n",
      "continued: 1\n",
      "deployment: 1\n",
      "8,500: 1\n",
      "march: 3\n",
      "came: 3\n",
      "ahead: 3\n",
      "anti-war: 1\n",
      "today: 1\n",
      "city: 13\n",
      "including: 8\n",
      "rome: 2\n",
      "paris: 7\n",
      "madrid: 1\n",
      "international: 8\n",
      "atomic: 1\n",
      "energy: 6\n",
      "agency: 13\n",
      "hold: 4\n",
      "second: 3\n",
      "day: 10\n",
      "talk: 18\n",
      "vienna: 1\n",
      "wednesday: 18\n",
      "respond: 1\n",
      "iran: 19\n",
      "resumption: 2\n",
      "low-level: 1\n",
      "uranium: 3\n",
      "conversion: 2\n",
      "week: 22\n",
      "restarted: 2\n",
      "part: 7\n",
      "process: 4\n",
      "isfahan: 1\n",
      "nuclear: 21\n",
      "plant: 6\n",
      "iranian: 3\n",
      "official: 43\n",
      "say: 99\n",
      "expect: 1\n",
      "get: 6\n",
      "access: 1\n",
      "sealed: 1\n",
      "sensitive: 1\n",
      "iaea: 1\n",
      "surveillance: 1\n",
      "system: 2\n",
      "begin: 8\n",
      "functioning: 1\n",
      "step: 3\n",
      "allow: 5\n",
      "facility: 3\n",
      "operate: 2\n",
      "full: 2\n",
      "capacity: 2\n",
      "european: 5\n",
      "union: 4\n",
      "u.s.: 33\n",
      "backing: 1\n",
      "threatened: 1\n",
      "refer: 1\n",
      "u.n.: 10\n",
      "security: 12\n",
      "council: 10\n",
      "could: 12\n",
      "impose: 1\n",
      "sanction: 2\n",
      "find: 1\n",
      "tehran: 6\n",
      "violated: 2\n",
      "non-proliferation: 1\n",
      "treaty: 1\n",
      "new: 15\n",
      "president: 40\n",
      "mahmoud: 5\n",
      "ahmadinejad: 2\n",
      "said: 65\n",
      "tuesday: 7\n",
      "incentive: 1\n",
      "aimed: 2\n",
      "persuading: 1\n",
      "end: 6\n",
      "fuel: 1\n",
      "program: 10\n",
      "insult: 1\n",
      "nation: 11\n",
      "two: 24\n",
      "german: 4\n",
      "four: 9\n",
      "nigerian: 1\n",
      "oil: 5\n",
      "worker: 4\n",
      "kidnapped: 3\n",
      "armed: 3\n",
      "militant: 18\n",
      "raid: 3\n",
      "boat: 2\n",
      "nigeria: 2\n",
      "oil-rich: 1\n",
      "delta: 3\n",
      "region: 13\n",
      "firm: 2\n",
      "bilfinger: 1\n",
      "berger: 1\n",
      "thomas: 1\n",
      "horbach: 1\n",
      "gunman: 3\n",
      "stopped: 1\n",
      "supply: 4\n",
      "sailed: 1\n",
      "state: 15\n",
      "bayelsa: 1\n",
      "inspect: 1\n",
      "offshore: 1\n",
      "field: 1\n",
      "owned: 1\n",
      "royal-dutch: 1\n",
      "shell: 4\n",
      "work: 3\n",
      "sub-contractor: 1\n",
      "group: 9\n",
      "frequently: 1\n",
      "attack: 25\n",
      "operation: 4\n",
      "niger: 2\n",
      "social: 1\n",
      "service: 5\n",
      "better: 2\n",
      "job: 5\n",
      "opportunity: 1\n",
      "multinational: 1\n",
      "company: 6\n",
      "poor: 2\n",
      "resident: 2\n",
      "often: 1\n",
      "complain: 1\n",
      "cheated: 1\n",
      "huge: 2\n",
      "rich: 1\n",
      "extracted: 1\n",
      "tribal: 5\n",
      "land: 2\n",
      "bulk: 1\n",
      "2.3: 1\n",
      "million: 6\n",
      "barrel: 1\n",
      "petroleum: 1\n",
      "pumped: 1\n",
      "daily: 2\n",
      "suspected: 4\n",
      "islamist: 3\n",
      "rebel: 12\n",
      "fired: 3\n",
      "mortar: 3\n",
      "palace: 3\n",
      "used: 2\n",
      "somalia: 7\n",
      "interim: 2\n",
      "abdullahi: 1\n",
      "yusuf: 1\n",
      "ahmad: 3\n",
      "immediately: 3\n",
      "clear: 3\n",
      "mogadishu: 7\n",
      "occurred: 5\n",
      "anyone: 1\n",
      "hurt: 3\n",
      "local: 1\n",
      "news: 8\n",
      "report: 27\n",
      "least: 14\n",
      "five: 6\n",
      "hit: 2\n",
      "compound: 1\n",
      "elsewhere: 3\n",
      "government: 26\n",
      "go: 4\n",
      "reconciliation: 1\n",
      "1,300: 1\n",
      "somali: 1\n",
      "elder: 2\n",
      "warlord: 1\n",
      "politician: 1\n",
      "invited: 1\n",
      "iraqi: 6\n",
      "military: 20\n",
      "tank: 2\n",
      "arrived: 2\n",
      "northern: 10\n",
      "mosul: 2\n",
      "offensive: 7\n",
      "al: 2\n",
      "qaida: 2\n",
      "fighter: 6\n",
      "many: 5\n",
      "sunni: 2\n",
      "arab: 3\n",
      "kurdish: 1\n",
      "last: 17\n",
      "34: 1\n",
      "people: 30\n",
      "wounded: 6\n",
      "200: 4\n",
      "commander: 3\n",
      "explained: 1\n",
      "american: 9\n",
      "force: 15\n",
      "participate: 1\n",
      "fled: 3\n",
      "successful: 1\n",
      "campaign: 3\n",
      "anbar: 1\n",
      "province: 10\n",
      "baghdad: 11\n",
      "largest: 3\n",
      "north: 8\n",
      "long: 3\n",
      "stronghold: 2\n",
      "violence: 4\n",
      "patrol: 1\n",
      "sunday: 13\n",
      "egyptian: 3\n",
      "arrested: 11\n",
      "16: 1\n",
      "member: 8\n",
      "opposition: 3\n",
      "muslim: 17\n",
      "brotherhood: 5\n",
      "prepare: 2\n",
      "parliamentary: 2\n",
      "runoff: 1\n",
      "election: 8\n",
      "saturday: 16\n",
      "arrest: 4\n",
      "friday: 17\n",
      "alexandria: 1\n",
      "spokesman: 7\n",
      "attempt: 4\n",
      "cut: 3\n",
      "supporter: 5\n",
      "punishment: 1\n",
      "winning: 1\n",
      "seat: 4\n",
      "earlier: 10\n",
      "tripled: 1\n",
      "strength: 1\n",
      "recent: 6\n",
      "raising: 2\n",
      "total: 2\n",
      "47: 1\n",
      "voter: 2\n",
      "cast: 1\n",
      "ballot: 1\n",
      "nine: 2\n",
      "candidate: 3\n",
      "majority: 3\n",
      "previous: 3\n",
      "round: 2\n",
      "voting: 1\n",
      "banned: 1\n",
      "political: 5\n",
      "endorses: 1\n",
      "so-called: 1\n",
      "independent: 2\n",
      "whose: 1\n",
      "allegiance: 1\n",
      "known: 5\n",
      "hardline: 1\n",
      "lawmaker: 1\n",
      "pakistan: 9\n",
      "west: 4\n",
      "frontier: 1\n",
      "pushed: 2\n",
      "law: 6\n",
      "aim: 1\n",
      "ensure: 1\n",
      "islamic: 8\n",
      "correctness: 1\n",
      "public: 4\n",
      "place: 5\n",
      "establishes: 1\n",
      "morality: 1\n",
      "enforce: 1\n",
      "decent: 1\n",
      "behavior: 3\n",
      "six-party: 2\n",
      "coalition: 4\n",
      "religious: 8\n",
      "based: 1\n",
      "mutahida: 1\n",
      "majlis-e-amal: 1\n",
      "dominates: 1\n",
      "provincial: 2\n",
      "assembly: 1\n",
      "bill: 6\n",
      "easily: 2\n",
      "passed: 1\n",
      "thursday: 10\n",
      "vote: 1\n",
      "68-34: 1\n",
      "governor: 1\n",
      "must: 2\n",
      "still: 5\n",
      "sign: 4\n",
      "becomes: 1\n",
      "seen: 1\n",
      "formality: 1\n",
      "proposed: 1\n",
      "call: 2\n",
      "setting: 1\n",
      "make: 6\n",
      "sure: 3\n",
      "adhere: 1\n",
      "value: 1\n",
      "entertainment: 1\n",
      "outlet: 1\n",
      "close: 2\n",
      "weekly: 1\n",
      "prayer: 1\n",
      "violator: 1\n",
      "jailed: 2\n",
      "six: 3\n",
      "month: 11\n",
      "denounced: 3\n",
      "measure: 4\n",
      "comparing: 1\n",
      "draconian: 1\n",
      "rule: 1\n",
      "former: 11\n",
      "taleban: 7\n",
      "neighboring: 4\n",
      "afghanistan: 7\n",
      "man: 8\n",
      "dressed: 2\n",
      "suicide: 4\n",
      "bomber: 5\n",
      "demonstration: 2\n",
      "publication: 1\n",
      "cartoon: 3\n",
      "depicting: 2\n",
      "islam: 1\n",
      "prophet: 4\n",
      "muhammad: 3\n",
      "bedfordshire: 1\n",
      "omar: 2\n",
      "khayam: 3\n",
      "bedford: 1\n",
      "breaching: 1\n",
      "condition: 4\n",
      "parole: 2\n",
      "home: 10\n",
      "office: 10\n",
      "sought: 3\n",
      "investigation: 2\n",
      "photographed: 1\n",
      "fatigue: 1\n",
      "black: 2\n",
      "cap: 1\n",
      "bulky: 1\n",
      "belt: 1\n",
      "told: 7\n",
      "associated: 2\n",
      "press: 3\n",
      "paroled: 1\n",
      "offender: 1\n",
      "give: 2\n",
      "cause: 3\n",
      "concern: 4\n",
      "sent: 3\n",
      "back: 2\n",
      "prison: 8\n",
      "ap: 1\n",
      "also: 19\n",
      "since: 9\n",
      "year: 33\n",
      "serving: 3\n",
      "half: 1\n",
      "six-year: 1\n",
      "sentence: 5\n",
      "drug: 5\n",
      "dealing: 2\n",
      "pakistani: 7\n",
      "unidentified: 1\n",
      "three: 11\n",
      "minister: 20\n",
      "semi-autonomous: 1\n",
      "bordering: 1\n",
      "prominent: 4\n",
      "leader: 13\n",
      "malik: 1\n",
      "faridullah: 1\n",
      "khan: 3\n",
      "traveling: 1\n",
      "south: 7\n",
      "waziristan: 3\n",
      "vehicle: 2\n",
      "ambushed: 1\n",
      "kani: 1\n",
      "wam: 1\n",
      "area: 7\n",
      "driver: 2\n",
      "responsibility: 5\n",
      "killing: 7\n",
      "ambush: 1\n",
      "army: 8\n",
      "almost: 2\n",
      "completely: 2\n",
      "eliminated: 1\n",
      "became: 2\n",
      "refuge: 1\n",
      "al-qaida: 2\n",
      "ousted: 1\n",
      "2001: 2\n",
      "senior: 2\n",
      "want: 4\n",
      "sordid: 1\n",
      "chapter: 1\n",
      "proliferation: 1\n",
      "top: 6\n",
      "scientist: 4\n",
      "behind: 2\n",
      "build: 4\n",
      "civilian: 4\n",
      "tie: 4\n",
      "united: 12\n",
      "ready: 2\n",
      "abdul: 1\n",
      "qadeer: 1\n",
      "available: 2\n",
      "direct: 2\n",
      "questioning: 2\n",
      "sale: 1\n",
      "secret: 2\n",
      "libya: 1\n",
      "korea: 5\n",
      "reason: 1\n",
      "national: 3\n",
      "sensitivity: 1\n",
      "making: 4\n",
      "giving: 1\n",
      "background: 2\n",
      "briefing: 1\n",
      "small: 3\n",
      "reporter: 3\n",
      "washington: 4\n",
      "admitted: 1\n",
      "2004: 2\n",
      "operated: 4\n",
      "worldwide: 2\n",
      "clandestine: 1\n",
      "network: 1\n",
      "sell: 2\n",
      "technology: 1\n",
      "market: 2\n",
      "placed: 1\n",
      "islamabad: 2\n",
      "considered: 3\n",
      "father: 2\n",
      "bomb: 3\n",
      "renew: 1\n",
      "controversial: 1\n",
      "multi-billion: 1\n",
      "dollar: 2\n",
      "contract: 2\n",
      "halliburton: 3\n",
      "provide: 6\n",
      "logistical: 1\n",
      "support: 2\n",
      "providing: 1\n",
      "list: 2\n",
      "meal: 1\n",
      "communication: 1\n",
      "several: 11\n",
      "critic: 2\n",
      "include: 3\n",
      "auditor: 1\n",
      "congressional: 1\n",
      "democrat: 3\n",
      "produced: 1\n",
      "shoddy: 1\n",
      "charge: 11\n",
      "much: 3\n",
      "money: 4\n",
      "strongly: 1\n",
      "denies: 3\n",
      "allegation: 2\n",
      "re-bidding: 1\n",
      "chance: 2\n",
      "compete: 1\n",
      "portion: 1\n",
      "representative: 3\n",
      "asia: 2\n",
      "pacific: 1\n",
      "economic: 4\n",
      "cooperation: 4\n",
      "business: 9\n",
      "advisory: 2\n",
      "holding: 2\n",
      "meeting: 11\n",
      "finalize: 1\n",
      "apec: 6\n",
      "summit: 6\n",
      "september: 1\n",
      "8: 1\n",
      "9: 1\n",
      "voa: 2\n",
      "nancy-amelia: 1\n",
      "collins: 1\n",
      "sydney: 1\n",
      "climate: 1\n",
      "change: 2\n",
      "world: 15\n",
      "trade: 5\n",
      "organization: 8\n",
      "stalled: 1\n",
      "negotiation: 7\n",
      "investment: 1\n",
      "expected: 9\n",
      "among: 4\n",
      "major: 5\n",
      "topic: 1\n",
      "abac: 7\n",
      "tim: 1\n",
      "harcourt: 4\n",
      "chief: 6\n",
      "economist: 1\n",
      "australian: 5\n",
      "commission: 3\n",
      "play: 1\n",
      "important: 2\n",
      "role: 4\n",
      "informing: 1\n",
      "problem: 4\n",
      "thing: 2\n",
      "tell: 1\n",
      "logjam: 1\n",
      "obstacle: 2\n",
      "improve: 2\n",
      "think: 2\n",
      "actually: 1\n",
      "played: 1\n",
      "pretty: 1\n",
      "good: 6\n",
      "leadership: 1\n",
      "talking: 1\n",
      "facilitation: 1\n",
      "basically: 1\n",
      "standard: 1\n",
      "consistent: 2\n",
      "harmonious: 1\n",
      "across: 6\n",
      "comprises: 1\n",
      "private: 1\n",
      "sector: 2\n",
      "21: 1\n",
      "economy: 2\n",
      "meet: 1\n",
      "time: 4\n",
      "made: 10\n",
      "permanent: 3\n",
      "body: 6\n",
      "1995: 6\n",
      "perspective: 1\n",
      "within: 2\n",
      "represent: 1\n",
      "range: 1\n",
      "medium: 9\n",
      "need: 2\n",
      "efficiency: 1\n",
      "encourage: 1\n",
      "conservation: 1\n",
      "practice: 1\n",
      "discus: 3\n",
      "way: 4\n",
      "enhance: 1\n",
      "regional: 2\n",
      "reckon: 1\n",
      "'ll: 2\n",
      "little: 2\n",
      "bit: 2\n",
      "custom: 2\n",
      "quarantine: 2\n",
      "arrangement: 1\n",
      "around: 5\n",
      "one-stop: 1\n",
      "shop: 1\n",
      "term: 3\n",
      "combining: 1\n",
      "immigration: 1\n",
      "together: 1\n",
      "",
      ": 1\n",
      "streamlined: 1\n",
      "provides: 3\n",
      "certainty: 1\n",
      "non-governmental: 1\n",
      "formal: 3\n",
      "dialogue: 1\n",
      "present: 1\n",
      "sudan: 7\n",
      "order: 1\n",
      "darfur: 4\n",
      "asking: 1\n",
      "foreign: 9\n",
      "mustafa: 1\n",
      "osman: 1\n",
      "ismail: 3\n",
      "sudanese: 8\n",
      "withdraw: 2\n",
      "position: 3\n",
      "held: 2\n",
      "april: 4\n",
      "cease-fire: 2\n",
      "western: 7\n",
      "agree: 1\n",
      "mr.: 27\n",
      "announced: 5\n",
      "decision: 4\n",
      "african: 6\n",
      "khartoum: 4\n",
      "au: 2\n",
      "repeatedly: 1\n",
      "truce: 2\n",
      "head: 8\n",
      "accused: 6\n",
      "helicopter: 2\n",
      "site: 5\n",
      "village: 2\n",
      "labado: 1\n",
      "defending: 1\n",
      "aid: 1\n",
      "relief: 3\n",
      "effort: 3\n",
      "suspended: 3\n",
      "due: 4\n",
      "indonesian: 6\n",
      "men: 9\n",
      "connection: 2\n",
      "october: 1\n",
      "1: 2\n",
      "bali: 5\n",
      "left: 2\n",
      "23: 2\n",
      "dead: 4\n",
      "flown: 2\n",
      "java: 1\n",
      "island: 3\n",
      "headquarters: 3\n",
      "french: 6\n",
      "cholily: 1\n",
      "captured: 1\n",
      "series: 2\n",
      "counter-terrorism: 1\n",
      "indonesia: 7\n",
      "ended: 2\n",
      "death: 10\n",
      "alleged: 3\n",
      "extremist: 2\n",
      "bombmaker: 1\n",
      "azahari: 2\n",
      "bin: 3\n",
      "husin: 2\n",
      "authority: 10\n",
      "blame: 1\n",
      "orchestrating: 1\n",
      "well: 4\n",
      "2002: 2\n",
      "shot: 3\n",
      "roman: 1\n",
      "catholic: 1\n",
      "nun: 2\n",
      "bodyguard: 1\n",
      "hospital: 3\n",
      "worked: 1\n",
      "islamist-controlled: 1\n",
      "witness: 3\n",
      "shooting: 2\n",
      "feared: 1\n",
      "linked: 1\n",
      "anger: 1\n",
      "toward: 2\n",
      "pope: 3\n",
      "benedict: 1\n",
      "pistol: 1\n",
      "attacked: 4\n",
      "sister: 1\n",
      "leonella: 1\n",
      "sgorbati: 1\n",
      "finished: 1\n",
      "teaching: 2\n",
      "medical: 5\n",
      "school: 6\n",
      "class: 1\n",
      "suspect: 3\n",
      "vatican: 1\n",
      "deplored: 1\n",
      "hoped: 2\n",
      "isolated: 1\n",
      "event: 2\n",
      "irrationality: 1\n",
      "arising: 1\n",
      "comment: 3\n",
      "angered: 1\n",
      "determined: 1\n",
      "motive: 1\n",
      "meant: 1\n",
      "offense: 2\n",
      "quoted: 1\n",
      "14: 5\n",
      "century: 1\n",
      "byzantine: 1\n",
      "emperor: 1\n",
      "saying: 6\n",
      "muhammed: 1\n",
      "brought: 1\n",
      "evil: 1\n",
      "targeted: 2\n",
      "northwest: 2\n",
      "third: 4\n",
      "launching: 2\n",
      "airstrikes: 1\n",
      "insurgent: 7\n",
      "gunships: 1\n",
      "pounded: 1\n",
      "hideout: 1\n",
      "orakzai: 3\n",
      "taliban: 2\n",
      "believed: 1\n",
      "avoid: 2\n",
      "nearby: 1\n",
      "launched: 3\n",
      "hunt: 1\n",
      "far: 2\n",
      "nearly: 6\n",
      "100: 1\n",
      "reported: 5\n",
      "dozen: 2\n",
      "stormed: 1\n",
      "checkpoint: 1\n",
      "32: 2\n",
      "counter-attack: 1\n",
      "found: 7\n",
      "kurram: 1\n",
      "along: 3\n",
      "afghan: 2\n",
      "border: 3\n",
      "ago: 3\n",
      "separate: 2\n",
      "clash: 2\n",
      "13: 1\n",
      "guerrilla: 2\n",
      "encounter: 1\n",
      "central: 4\n",
      "uruzgan: 1\n",
      "others: 5\n",
      "injured: 2\n",
      "fighting: 2\n",
      "another: 8\n",
      "eastern: 4\n",
      "paktika: 1\n",
      "separately: 1\n",
      "nato-led: 1\n",
      "peacekeeping: 1\n",
      "mission: 3\n",
      "early: 6\n",
      "mazar-e-sharif: 1\n",
      "motivated: 2\n",
      "spared: 1\n",
      "bloodshed: 1\n",
      "plagued: 1\n",
      "russian: 4\n",
      "died: 9\n",
      "wave: 2\n",
      "extremely: 1\n",
      "cold: 2\n",
      "weather: 3\n",
      "gripping: 1\n",
      "bringing: 2\n",
      "toll: 6\n",
      "43: 1\n",
      "past: 3\n",
      "emergency: 3\n",
      "victim: 3\n",
      "moscow: 3\n",
      "exposure: 1\n",
      "19: 1\n",
      "hospitalized: 2\n",
      "hypothermia: 1\n",
      "russia: 3\n",
      "itar-tass: 1\n",
      "quote: 2\n",
      "intoxicated: 1\n",
      "homeless: 1\n",
      "unusually: 1\n",
      "affecting: 1\n",
      "baltic: 1\n",
      "lithuania: 1\n",
      "latvia: 1\n",
      "estonia: 1\n",
      "moving: 1\n",
      "nordic: 1\n",
      "rise: 1\n",
      "temperature: 1\n",
      "continue: 2\n",
      "hover: 1\n",
      "minus: 1\n",
      "30: 4\n",
      "degree: 1\n",
      "celsius: 1\n",
      "lower: 1\n",
      "overnight: 1\n",
      "power: 5\n",
      "rationing: 1\n",
      "effect: 1\n",
      "health: 10\n",
      "vietnam: 3\n",
      "deadly: 3\n",
      "strain: 2\n",
      "bird: 8\n",
      "flu: 9\n",
      "virus: 3\n",
      "vietnamese: 1\n",
      "50: 4\n",
      "27-year: 1\n",
      "old: 3\n",
      "ninh: 1\n",
      "binh: 1\n",
      "late: 2\n",
      "tested: 1\n",
      "positive: 2\n",
      "h5n1: 3\n",
      "fell: 1\n",
      "ill: 1\n",
      "slaughtering: 1\n",
      "chicken: 2\n",
      "person: 3\n",
      "die: 1\n",
      "avian: 3\n",
      "influenza: 1\n",
      "counting: 1\n",
      "latest: 4\n",
      "227: 1\n",
      "2003: 3\n",
      "case: 4\n",
      "103: 1\n",
      "burned: 1\n",
      "dragged: 1\n",
      "street: 4\n",
      "ethiopian: 4\n",
      "seven: 3\n",
      "erupted: 1\n",
      "rolling: 1\n",
      "near: 8\n",
      "defense: 9\n",
      "ministry: 7\n",
      "returned: 2\n",
      "heavy: 2\n",
      "fire: 3\n",
      "ethiopia: 2\n",
      "deployed: 2\n",
      "december: 2\n",
      "help: 5\n",
      "push: 1\n",
      "movement: 3\n",
      "internationally-recognized: 1\n",
      "struggling: 1\n",
      "contain: 1\n",
      "regular: 2\n",
      "outburst: 1\n",
      "loyal: 1\n",
      "fallen: 1\n",
      "replace: 1\n",
      "addis: 1\n",
      "ababa: 1\n",
      "plan: 10\n",
      "palestinian: 11\n",
      "negotiator: 2\n",
      "ahmed: 1\n",
      "qureia: 3\n",
      "israeli: 12\n",
      "mediator: 1\n",
      "agreed: 4\n",
      "document: 4\n",
      "outlining: 1\n",
      "progress: 4\n",
      "peace: 13\n",
      "accord: 2\n",
      "remark: 3\n",
      "side: 5\n",
      "writing: 1\n",
      "issue: 5\n",
      "discussed: 1\n",
      "elaborate: 1\n",
      "prime: 7\n",
      "ehud: 1\n",
      "olmert: 1\n",
      "abbas: 4\n",
      "resumed: 1\n",
      "u.s.-brokered: 1\n",
      "november: 3\n",
      "slow: 1\n",
      "expressed: 1\n",
      "hope: 5\n",
      "reaching: 2\n",
      "deal: 6\n",
      "george: 1\n",
      "leaf: 3\n",
      "next: 3\n",
      "agreement: 7\n",
      "israel: 4\n",
      "take: 1\n",
      "miracle: 1\n",
      "bank: 8\n",
      "room: 1\n",
      "israeli-palestinian: 1\n",
      "general: 6\n",
      "udi: 1\n",
      "adam: 4\n",
      "command: 1\n",
      "resignation: 1\n",
      "statement: 7\n",
      "asked: 1\n",
      "leave: 5\n",
      "post: 1\n",
      "soon: 1\n",
      "possible: 4\n",
      "staff: 5\n",
      "accepted: 1\n",
      "request: 3\n",
      "replacement: 1\n",
      "widely: 1\n",
      "aside: 1\n",
      "34-day: 1\n",
      "hezbollah: 1\n",
      "lebanon: 2\n",
      "replaced: 1\n",
      "coordinator: 1\n",
      "disagreement: 1\n",
      "lieutenant: 1\n",
      "dan: 1\n",
      "halutz: 1\n",
      "conduct: 1\n",
      "washington-based: 1\n",
      "research: 1\n",
      "institute: 2\n",
      "building: 1\n",
      "reactor: 4\n",
      "produce: 2\n",
      "enough: 1\n",
      "plutonium: 3\n",
      "40: 3\n",
      "weapon: 8\n",
      "science: 1\n",
      "satellite: 2\n",
      "photo: 4\n",
      "show: 5\n",
      "construction: 1\n",
      "larger: 1\n",
      "khushab: 2\n",
      "district: 1\n",
      "punjab: 1\n",
      "kilogram: 1\n",
      "weapons-grade: 1\n",
      "according: 3\n",
      "capable: 1\n",
      "producing: 1\n",
      "warhead: 1\n",
      "spokeswoman: 3\n",
      "tasnim: 1\n",
      "aslam: 1\n",
      "declined: 2\n",
      "whether: 1\n",
      "constructed: 1\n",
      "presence: 1\n",
      "diabetes: 2\n",
      "association: 2\n",
      "disease: 5\n",
      "leading: 2\n",
      "blindness: 1\n",
      "adult: 2\n",
      "kidney: 1\n",
      "failure: 1\n",
      "rate: 3\n",
      "amputation: 1\n",
      "10: 3\n",
      "higher: 1\n",
      "suffer: 1\n",
      "expert: 3\n",
      "learn: 1\n",
      "manage: 1\n",
      "live: 1\n",
      "healthier: 1\n",
      "normal: 2\n",
      "life: 4\n",
      "june: 3\n",
      "soh: 1\n",
      "camp: 2\n",
      "child: 4\n",
      "chronic: 1\n",
      "approach: 1\n",
      "living: 5\n",
      "letting: 1\n",
      "kid: 1\n",
      "amy: 1\n",
      "katz: 1\n",
      "narrates: 1\n",
      "cuba: 3\n",
      "panama: 4\n",
      "restore: 1\n",
      "consular: 2\n",
      "relation: 5\n",
      "havana: 1\n",
      "broke: 1\n",
      "pardoning: 1\n",
      "convicted: 6\n",
      "assassination: 1\n",
      "cuban: 2\n",
      "fidel: 1\n",
      "castro: 2\n",
      "reopen: 1\n",
      "consulate: 4\n",
      "following: 1\n",
      "panamanian: 2\n",
      "martin: 1\n",
      "torrijos: 1\n",
      "vice: 1\n",
      "carlos: 1\n",
      "lage: 1\n",
      "took: 6\n",
      "sideline: 1\n",
      "ibero-american: 2\n",
      "san: 1\n",
      "jose: 1\n",
      "costa: 1\n",
      "rica: 1\n",
      "severed: 1\n",
      "august: 4\n",
      "hour: 4\n",
      "mireya: 1\n",
      "moscoso: 1\n",
      "final: 3\n",
      "pardoned: 1\n",
      "preventing: 1\n",
      "extradition: 3\n",
      "plotting: 1\n",
      "kill: 1\n",
      "2000: 3\n",
      "youth: 1\n",
      "gaza: 2\n",
      "strip: 1\n",
      "collecting: 1\n",
      "rocket: 2\n",
      "launcher: 1\n",
      "recently: 2\n",
      "source: 3\n",
      "strike: 2\n",
      "teenager: 1\n",
      "began: 5\n",
      "haniyeh: 1\n",
      "hamas-led: 1\n",
      "recognize: 2\n",
      "problematic: 1\n",
      "requires: 1\n",
      "exchange: 1\n",
      "pullout: 1\n",
      "territory: 1\n",
      "spoke: 2\n",
      "infighting: 1\n",
      "hamas: 1\n",
      "rival: 2\n",
      "fatah: 1\n",
      "automaker: 1\n",
      "chrysler: 4\n",
      "opened: 2\n",
      "570: 1\n",
      "engine: 3\n",
      "mexico: 3\n",
      "ceremonial: 2\n",
      "startup: 1\n",
      "saltillo: 1\n",
      "mexican: 2\n",
      "felipe: 1\n",
      "calderon: 2\n",
      "sixth: 2\n",
      "create: 1\n",
      "700: 1\n",
      "become: 3\n",
      "auto: 1\n",
      "industry: 4\n",
      "fuel-efficient: 1\n",
      "pentastar: 1\n",
      "v-6: 1\n",
      "dodge: 1\n",
      "jeep: 1\n",
      "ram: 1\n",
      "4,40,000: 1\n",
      "per: 1\n",
      "jury: 2\n",
      "maker: 1\n",
      "merck: 7\n",
      "liable: 1\n",
      "heart: 4\n",
      "suffered: 2\n",
      "taking: 3\n",
      "painkiller: 1\n",
      "vioxx: 5\n",
      "jersey: 1\n",
      "jurist: 1\n",
      "stress: 1\n",
      "risk: 4\n",
      "caused: 4\n",
      "60-year: 1\n",
      "postal: 1\n",
      "plaintiff: 1\n",
      "argued: 1\n",
      "responsible: 4\n",
      "rejected: 1\n",
      "claim: 2\n",
      "failed: 2\n",
      "properly: 1\n",
      "warn: 1\n",
      "user: 3\n",
      "withdrew: 1\n",
      "popular: 2\n",
      "study: 1\n",
      "showed: 3\n",
      "doubled: 1\n",
      "long-term: 2\n",
      "facing: 3\n",
      "lawsuit: 1\n",
      "verdict: 1\n",
      "first: 8\n",
      "ordered: 1\n",
      "pay: 2\n",
      "widow: 1\n",
      "appealing: 1\n",
      "rumor: 1\n",
      "true: 2\n",
      "nicole: 1\n",
      "ritchie: 2\n",
      "pregnant: 1\n",
      "speaking: 1\n",
      "abc: 1\n",
      "interviewer: 1\n",
      "dianne: 1\n",
      "sawyer: 1\n",
      "25-year-old: 1\n",
      "co-star: 1\n",
      "tv: 2\n",
      "simple: 1\n",
      "pregnancy: 1\n",
      "boyfriend: 1\n",
      "joel: 1\n",
      "madden: 1\n",
      "rock: 2\n",
      "band: 1\n",
      "charlotte: 1\n",
      "guilty: 1\n",
      "plea: 1\n",
      "driving: 2\n",
      "influence: 1\n",
      "resulting: 1\n",
      "four-day: 1\n",
      "jail: 1\n",
      "something: 1\n",
      "wrong: 1\n",
      "personally: 1\n",
      "apologize: 1\n",
      "every: 2\n",
      "single: 1\n",
      "lost: 3\n",
      "loved: 1\n",
      "drunk: 1\n",
      "would: 9\n",
      "unfortunately: 1\n",
      "ca: 1\n",
      "n't: 1\n",
      "paying: 1\n",
      "interview: 5\n",
      "air: 3\n",
      "2: 1\n",
      "3: 3\n",
      "morning: 1\n",
      "america: 1\n",
      "later: 1\n",
      "20/20: 1\n",
      "senator: 2\n",
      "john: 2\n",
      "warner: 2\n",
      "southeastern: 1\n",
      "virginia: 2\n",
      "republican: 3\n",
      "figure: 1\n",
      "debate: 1\n",
      "retire: 1\n",
      "finishing: 1\n",
      "2009: 1\n",
      "outside: 3\n",
      "university: 1\n",
      "seek: 1\n",
      "2008: 2\n",
      "chairman: 3\n",
      "powerful: 1\n",
      "senate: 3\n",
      "committee: 2\n",
      "openly: 1\n",
      "criticized: 1\n",
      "handling: 1\n",
      "called: 5\n",
      "withdrawing: 1\n",
      "retirement: 1\n",
      "open: 4\n",
      "relatively: 1\n",
      "safe: 2\n",
      "fight: 1\n",
      "control: 2\n",
      "protect: 1\n",
      "expand: 1\n",
      "one-seat: 1\n",
      "80-year-old: 1\n",
      "navy: 1\n",
      "secretary: 3\n",
      "served: 2\n",
      "indian: 1\n",
      "ocean: 1\n",
      "tsunami: 2\n",
      "520: 1\n",
      "damage: 2\n",
      "fishing: 5\n",
      "worst: 2\n",
      "food: 2\n",
      "agricultural: 1\n",
      "fao: 2\n",
      "destroyed: 1\n",
      "damaged: 1\n",
      "1,11,000: 2\n",
      "vessel: 1\n",
      "loss: 3\n",
      "significant: 1\n",
      "vital: 2\n",
      "estimate: 2\n",
      "includes: 1\n",
      "maldives: 1\n",
      "sri: 4\n",
      "lanka: 2\n",
      "thailand: 5\n",
      "rebuild: 1\n",
      "infrastructure: 1\n",
      "disaster: 1\n",
      "developing: 2\n",
      "strategy: 1\n",
      "recovery: 1\n",
      "96: 1\n",
      "outbreak: 1\n",
      "started: 2\n",
      "16-year-old: 1\n",
      "girl: 5\n",
      "town: 4\n",
      "bekasi: 1\n",
      "outskirt: 2\n",
      "jakarta: 2\n",
      "monday: 14\n",
      "32-year-old: 1\n",
      "woman: 3\n",
      "tangerang: 1\n",
      "kept: 1\n",
      "backyard: 1\n",
      "human: 5\n",
      "usually: 1\n",
      "infected: 2\n",
      "contact: 2\n",
      "poultry: 1\n",
      "fear: 2\n",
      "may: 2\n",
      "mutate: 1\n",
      "form: 4\n",
      "transmitted: 1\n",
      "mutation: 1\n",
      "spark: 1\n",
      "global: 1\n",
      "pandemic: 1\n",
      "potential: 1\n",
      "evidence: 3\n",
      "prove: 2\n",
      "muslim-majority: 1\n",
      "training: 2\n",
      "malaysia: 7\n",
      "thai: 2\n",
      "deputy: 1\n",
      "interior: 2\n",
      "sutham: 1\n",
      "saengprathum: 1\n",
      "kelantan: 1\n",
      "see: 1\n",
      "bangkok: 2\n",
      "thaksin: 1\n",
      "shinawatra: 1\n",
      "belief: 1\n",
      "trained: 1\n",
      "demanded: 1\n",
      "proof: 1\n",
      "500: 1\n",
      "insurgency: 2\n",
      "supported: 1\n",
      "anti-japanese: 2\n",
      "chinese: 7\n",
      "shanghai: 2\n",
      "turned: 1\n",
      "violent: 1\n",
      "pelting: 1\n",
      "japanese: 4\n",
      "bottle: 1\n",
      "egg: 1\n",
      "japan: 6\n",
      "bid: 3\n",
      "tokyo: 4\n",
      "downplaying: 1\n",
      "atrocity: 1\n",
      "beijing: 3\n",
      "waiting: 2\n",
      "capital: 6\n",
      "councilor: 1\n",
      "tang: 1\n",
      "jiaxuan: 1\n",
      "xinhua: 3\n",
      "china: 13\n",
      "urging: 1\n",
      "calm: 2\n",
      "orderly: 1\n",
      "manner: 1\n",
      "nobutaka: 1\n",
      "machimura: 1\n",
      "arrives: 1\n",
      "counterpart: 1\n",
      "li: 1\n",
      "zhaoxing: 1\n",
      "2012: 2\n",
      "summer: 2\n",
      "olympics: 1\n",
      "promised: 2\n",
      "greenest: 1\n",
      "game: 3\n",
      "history: 2\n",
      "soothe: 1\n",
      "rising: 1\n",
      "cost: 4\n",
      "2,012: 1\n",
      "design: 1\n",
      "champion: 1\n",
      "low: 2\n",
      "waste: 1\n",
      "carbon: 1\n",
      "emission: 2\n",
      "environmentally: 1\n",
      "friendly: 1\n",
      "transportation: 1\n",
      "olympic: 4\n",
      "delivery: 1\n",
      "percent: 2\n",
      "generating: 1\n",
      "using: 1\n",
      "renewable: 1\n",
      "tony: 1\n",
      "blair: 1\n",
      "farther: 1\n",
      "preparation: 1\n",
      "stage: 1\n",
      "host: 1\n",
      "budget: 1\n",
      "finalized: 1\n",
      "already: 4\n",
      "risen: 1\n",
      "substantially: 1\n",
      "july: 7\n",
      "2005: 2\n",
      "select: 1\n",
      "highly: 1\n",
      "critical: 2\n",
      "financing: 1\n",
      "giant: 2\n",
      "panda: 4\n",
      "endangered: 1\n",
      "specie: 1\n",
      "1,600: 1\n",
      "wild: 1\n",
      "mascot: 1\n",
      "modeled: 1\n",
      "jing: 4\n",
      "conservationist: 1\n",
      "draw: 1\n",
      "attention: 1\n",
      "threat: 4\n",
      "--: 1\n",
      "symbol: 1\n",
      "sam: 1\n",
      "beattie: 1\n",
      "sichuan: 1\n",
      "personnel: 1\n",
      "crash: 3\n",
      "nato: 4\n",
      "aircraft: 3\n",
      "went: 1\n",
      "radar: 1\n",
      "crashed: 1\n",
      "kandahar: 2\n",
      "supporting: 1\n",
      "indication: 1\n",
      "enemy: 2\n",
      "action: 3\n",
      "causing: 1\n",
      "drive: 1\n",
      "remnant: 1\n",
      "medusa: 1\n",
      "removing: 1\n",
      "stability: 1\n",
      "reconstruction: 1\n",
      "development: 3\n",
      "achieved: 1\n",
      "lankan: 2\n",
      "tamil: 2\n",
      "journalist: 1\n",
      "colombo: 1\n",
      "abducted: 1\n",
      "attacker: 1\n",
      "restaurant: 1\n",
      "dharmeratnam: 1\n",
      "sivaram: 3\n",
      "board: 1\n",
      "pro-rebel: 1\n",
      "tamilnet: 1\n",
      "web: 2\n",
      "columnist: 1\n",
      "newspaper: 2\n",
      "mirror: 1\n",
      "ongoing: 1\n",
      "civil: 1\n",
      "tiger: 1\n",
      "brutally: 1\n",
      "lake: 1\n",
      "gagged: 1\n",
      "gunshot: 1\n",
      "wound: 1\n",
      "considering: 1\n",
      "bilateral: 2\n",
      "normalize: 1\n",
      "arrange: 1\n",
      "ambition: 1\n",
      "resume: 4\n",
      "pyongyang: 1\n",
      "odds: 1\n",
      "kidnapping: 1\n",
      "citizen: 4\n",
      "1970s: 1\n",
      "1980s: 1\n",
      "right: 3\n",
      "failing: 2\n",
      "secure: 1\n",
      "upcoming: 1\n",
      "crime: 7\n",
      "trial: 6\n",
      "saddam: 4\n",
      "hussein: 3\n",
      "watch: 1\n",
      "released: 4\n",
      "stealing: 1\n",
      "invasion: 1\n",
      "damaging: 1\n",
      "mass: 1\n",
      "graf: 1\n",
      "war-torn: 1\n",
      "department: 3\n",
      "reviewed: 1\n",
      "arraigned: 1\n",
      "base: 1\n",
      "face: 3\n",
      "genocide: 3\n",
      "humanity: 2\n",
      "egypt: 4\n",
      "different: 1\n",
      "receiving: 1\n",
      "treatment: 1\n",
      "stable: 2\n",
      "thirty: 1\n",
      "contracted: 1\n",
      "thirteen: 1\n",
      "oil-for-food: 4\n",
      "illegally: 1\n",
      "obtained: 2\n",
      "smuggling: 3\n",
      "corruption: 2\n",
      "federal: 2\n",
      "reserve: 1\n",
      "paul: 2\n",
      "volcker: 2\n",
      "start: 5\n",
      "government-funded: 1\n",
      "alhurra: 1\n",
      "television: 2\n",
      "station: 2\n",
      "lot: 1\n",
      "confusion: 1\n",
      "refused: 2\n",
      "specific: 1\n",
      "large: 1\n",
      "amount: 2\n",
      "fund: 3\n",
      "diverted: 1\n",
      "created: 1\n",
      "gulf: 1\n",
      "use: 2\n",
      "profit: 1\n",
      "humanitarian: 1\n",
      "kyrgyzstan: 1\n",
      "justice: 1\n",
      "tynychbek: 1\n",
      "akmatbayev: 2\n",
      "bishkek: 2\n",
      "inmate: 2\n",
      "hostage: 1\n",
      "murat: 1\n",
      "sutalinov: 1\n",
      "gave: 1\n",
      "detail: 4\n",
      "hostage-takers: 1\n",
      "involved: 1\n",
      "kyrgyz: 1\n",
      "riot: 1\n",
      "forced: 1\n",
      "evacuate: 1\n",
      "remained: 1\n",
      "surrounded: 1\n",
      "commercial: 1\n",
      "airplane: 1\n",
      "europe: 4\n",
      "landed: 1\n",
      "20: 2\n",
      "plane: 1\n",
      "france: 8\n",
      "aigle: 2\n",
      "azur: 2\n",
      "airline: 4\n",
      "charles: 1\n",
      "de: 1\n",
      "gaulle: 1\n",
      "airport: 2\n",
      "touched: 1\n",
      "flight: 2\n",
      "largely: 1\n",
      "passenger: 2\n",
      "diplomat: 1\n",
      "anne-marie: 1\n",
      "idrac: 1\n",
      "historic: 1\n",
      "flying: 2\n",
      "etihad: 1\n",
      "emirate: 2\n",
      "operating: 1\n",
      "added: 1\n",
      "moderate: 2\n",
      "january: 5\n",
      "issued: 3\n",
      "unemployment: 1\n",
      "increased: 1\n",
      "4.6: 1\n",
      "earned: 1\n",
      "average: 1\n",
      "17.09: 1\n",
      "slight: 1\n",
      "increase: 1\n",
      "analyst: 1\n",
      "ease: 2\n",
      "worry: 1\n",
      "tight: 1\n",
      "significantly: 1\n",
      "inflate: 1\n",
      "wage: 1\n",
      "decided: 1\n",
      "maintain: 1\n",
      "key: 2\n",
      "interest: 1\n",
      "despite: 2\n",
      "inflation: 1\n",
      "venezuelan: 1\n",
      "hugo: 1\n",
      "chavez: 4\n",
      "colombia: 2\n",
      "pawn: 1\n",
      "reacted: 1\n",
      "sharply: 1\n",
      "colombian: 3\n",
      "alvaro: 1\n",
      "uribe: 2\n",
      "met: 1\n",
      "try: 1\n",
      "mend: 1\n",
      "juan: 1\n",
      "manuel: 1\n",
      "santos: 1\n",
      "venezuela: 3\n",
      "follow: 1\n",
      "promise: 1\n",
      "televised: 2\n",
      "speech: 1\n",
      "tension: 1\n",
      "rose: 3\n",
      "ecuador: 1\n",
      "responded: 1\n",
      "sending: 1\n",
      "lashed: 1\n",
      "policy: 2\n",
      "freedom: 6\n",
      "criticism: 2\n",
      "harm: 1\n",
      "continues: 1\n",
      "pervasive: 1\n",
      "severe: 2\n",
      "violation: 2\n",
      "regularly: 1\n",
      "imprisoning: 1\n",
      "harassing: 1\n",
      "practitioner: 1\n",
      "liu: 2\n",
      "jianchao: 1\n",
      "attempting: 1\n",
      "interfere: 1\n",
      "internal: 1\n",
      "affair: 1\n",
      "guise: 1\n",
      "run: 3\n",
      "counter: 1\n",
      "u.s: 1\n",
      "protects: 1\n",
      "enjoy: 1\n",
      "religion: 1\n",
      "communist: 1\n",
      "allows: 1\n",
      "worship: 2\n",
      "state-approved: 1\n",
      "state-monitored: 1\n",
      "church: 1\n",
      "temple: 1\n",
      "mosque: 1\n",
      "unauthorized: 1\n",
      "subject: 1\n",
      "pardon: 6\n",
      "criminal: 2\n",
      "commuted: 2\n",
      "high-profile: 1\n",
      "name: 1\n",
      "act: 1\n",
      "embezzlement: 1\n",
      "false: 2\n",
      "unlawfully: 1\n",
      "wildlife: 1\n",
      "committing: 1\n",
      "granted: 3\n",
      "171: 1\n",
      "eight: 3\n",
      "absolute: 1\n",
      "typical: 1\n",
      "prepares: 1\n",
      "white: 2\n",
      "case-by-case: 1\n",
      "basis: 1\n",
      "review: 2\n",
      "clemency: 1\n",
      "uganda: 4\n",
      "lord: 1\n",
      "resistance: 1\n",
      "nearing: 1\n",
      "signed: 5\n",
      "disarmament: 1\n",
      "demobilization: 1\n",
      "lra: 3\n",
      "however: 1\n",
      "uncertainty: 1\n",
      "joseph: 1\n",
      "kony: 2\n",
      "immunity: 1\n",
      "prosecution: 1\n",
      "court: 4\n",
      "murdering: 1\n",
      "raping: 1\n",
      "mutilating: 1\n",
      "tennis: 2\n",
      "player: 1\n",
      "roger: 1\n",
      "federer: 2\n",
      "unbeatable: 1\n",
      "warmup: 1\n",
      "abruptly: 1\n",
      "rare: 1\n",
      "swiss: 1\n",
      "star: 1\n",
      "upset: 1\n",
      "tommy: 1\n",
      "haas: 2\n",
      "opening: 1\n",
      "match: 2\n",
      "kooyong: 1\n",
      "classic: 1\n",
      "melbourne: 1\n",
      "playing: 1\n",
      "exhibition: 1\n",
      "tournament: 1\n",
      "count: 1\n",
      "professional: 1\n",
      "tour: 1\n",
      "record: 1\n",
      "41st: 1\n",
      "ranked: 1\n",
      "fended: 1\n",
      "break: 1\n",
      "point: 1\n",
      "closed: 3\n",
      "3-jun: 1\n",
      "6-apr: 2\n",
      "win: 1\n",
      "ace: 1\n",
      "healthy: 1\n",
      "ankle: 1\n",
      "curtailed: 1\n",
      "season: 1\n",
      "flare: 1\n",
      "space: 6\n",
      "received: 1\n",
      "image: 1\n",
      "scientific: 1\n",
      "reading: 1\n",
      "surface: 3\n",
      "saturn: 2\n",
      "moon: 3\n",
      "titan: 1\n",
      "germany: 1\n",
      "information: 3\n",
      "taken: 1\n",
      "huygens: 2\n",
      "probe: 2\n",
      "look: 1\n",
      "like: 2\n",
      "drainage: 1\n",
      "channel: 1\n",
      "canyon: 1\n",
      "likely: 2\n",
      "type: 1\n",
      "liquid: 1\n",
      "transmitting: 1\n",
      "data: 1\n",
      "cassini: 1\n",
      "spacecraft: 1\n",
      "landing: 1\n",
      "jointly: 1\n",
      "italian: 1\n",
      "clue: 1\n",
      "primitive: 1\n",
      "earth: 2\n",
      "evolved: 1\n",
      "life-bearing: 1\n",
      "planet: 1\n",
      "ugandan: 2\n",
      "kizza: 1\n",
      "besigye: 5\n",
      "bail: 1\n",
      "judge: 2\n",
      "ruled: 2\n",
      "detention: 2\n",
      "illegal: 3\n",
      "tear: 1\n",
      "gas: 7\n",
      "disperse: 1\n",
      "hundred: 2\n",
      "dr.: 4\n",
      "kampala: 1\n",
      "release: 1\n",
      "detained: 2\n",
      "self-imposed: 1\n",
      "exile: 1\n",
      "yoweri: 1\n",
      "museveni: 1\n",
      "presidential: 2\n",
      "tribunal: 3\n",
      "charged: 2\n",
      "terrorism: 2\n",
      "possessing: 1\n",
      "high: 2\n",
      "bosco: 1\n",
      "katutsi: 1\n",
      "rape: 1\n",
      "denied: 2\n",
      "trumped: 1\n",
      "keep: 1\n",
      "running: 1\n",
      "barack: 2\n",
      "obama: 8\n",
      "improvement: 1\n",
      "care: 3\n",
      "inevitable: 1\n",
      "garden: 1\n",
      "nominated: 1\n",
      "african-american: 1\n",
      "regina: 1\n",
      "benjamin: 1\n",
      "surgeon: 1\n",
      "naysayer: 1\n",
      "cynic: 1\n",
      "bet: 1\n",
      "passage: 1\n",
      "insurance: 1\n",
      "legislation: 1\n",
      "inaction: 1\n",
      "option: 2\n",
      "vowed: 2\n",
      "unveil: 1\n",
      "reform: 1\n",
      "controversy: 1\n",
      "extending: 1\n",
      "coverage: 1\n",
      "uninsured: 1\n",
      "center: 2\n",
      "trillion: 1\n",
      "approved: 5\n",
      "debt: 4\n",
      "17: 3\n",
      "latin: 1\n",
      "37: 1\n",
      "billion: 1\n",
      "wolfowitz: 1\n",
      "two-thirds: 1\n",
      "meaning: 1\n",
      "forgiving: 1\n",
      "move: 2\n",
      "follows: 2\n",
      "pledge: 1\n",
      "wealthy: 1\n",
      "g-8: 1\n",
      "cancel: 1\n",
      "poorest: 1\n",
      "located: 2\n",
      "africa: 1\n",
      "eligible: 1\n",
      "benin: 1\n",
      "bolivia: 1\n",
      "burkina: 1\n",
      "faso: 1\n",
      "ghana: 1\n",
      "guyana: 1\n",
      "honduras: 1\n",
      "madagascar: 1\n",
      "mali: 1\n",
      "mozambique: 1\n",
      "nicaragua: 1\n",
      "rwanda: 1\n",
      "senegal: 1\n",
      "tanzania: 1\n",
      "zambia: 2\n",
      "wind-blown: 1\n",
      "dust: 2\n",
      "polluted: 1\n",
      "declared: 2\n",
      "holiday: 1\n",
      "domestic: 1\n",
      "canceled: 1\n",
      "advising: 1\n",
      "respiratory: 1\n",
      "stay: 1\n",
      "inside: 2\n",
      "quality: 1\n",
      "potentially: 1\n",
      "harmful: 1\n",
      "particulate: 1\n",
      "reached: 4\n",
      "dangerous: 2\n",
      "level: 2\n",
      "blown: 1\n",
      "saudi: 1\n",
      "arabia: 1\n",
      "sandstorm: 2\n",
      "blanketed: 1\n",
      "wind: 1\n",
      "blowing: 1\n",
      "desert: 1\n",
      "fertile: 1\n",
      "soil: 1\n",
      "greenery: 1\n",
      "shortly: 1\n",
      "midnight: 1\n",
      "blew: 1\n",
      "crowd: 1\n",
      "young: 1\n",
      "beachfront: 1\n",
      "nightclub: 1\n",
      "tel: 1\n",
      "aviv: 1\n",
      "spotted: 1\n",
      "prevented: 1\n",
      "entering: 1\n",
      "club: 1\n",
      "gotten: 1\n",
      "carnage: 1\n",
      "even: 1\n",
      "worse: 1\n",
      "unclear: 1\n",
      "nothing: 1\n",
      "ramallah: 1\n",
      "track: 1\n",
      "punish: 1\n",
      "sabotage: 1\n",
      "shatters: 1\n",
      "ariel: 1\n",
      "sharon: 1\n",
      "initial: 1\n",
      "launch: 2\n",
      "remote: 1\n",
      "biak: 1\n",
      "coast: 1\n",
      "papua: 1\n",
      "guinea: 1\n",
      "preliminary: 1\n",
      "well-suited: 1\n",
      "project: 2\n",
      "proximity: 1\n",
      "equator: 1\n",
      "easier: 1\n",
      "orbit: 1\n",
      "susilo: 1\n",
      "bambang: 1\n",
      "yudhoyono: 1\n",
      "scheduled: 2\n",
      "visit: 3\n",
      "algerian: 2\n",
      "subway: 2\n",
      "prosecutor: 2\n",
      "rachid: 1\n",
      "ramda: 5\n",
      "helped: 1\n",
      "metro: 1\n",
      "38-year-old: 1\n",
      "10-year: 1\n",
      "investigator: 1\n",
      "helping: 1\n",
      "fought: 1\n",
      "accomplice: 1\n",
      "25: 2\n",
      "150: 1\n",
      "refugee: 8\n",
      "begun: 1\n",
      "returning: 1\n",
      "republic: 3\n",
      "16,000: 1\n",
      "car: 3\n",
      "voluntarily: 1\n",
      "return: 5\n",
      "fly: 2\n",
      "rest: 1\n",
      "involving: 1\n",
      "neighbor: 1\n",
      "13,000: 1\n",
      "democratic: 1\n",
      "congo: 1\n",
      "7,000: 1\n",
      "congolese: 1\n",
      "currently: 2\n",
      "teacher: 1\n",
      "stabbed: 1\n",
      "12-year-old: 1\n",
      "le: 1\n",
      "yu: 1\n",
      "hagino: 1\n",
      "murder: 3\n",
      "scene: 1\n",
      "uji: 1\n",
      "confessed: 1\n",
      "stabbing: 1\n",
      "knife: 1\n",
      "arguing: 1\n",
      "incident: 1\n",
      "walked: 1\n",
      "alone: 1\n",
      "elementary: 1\n",
      "deserted: 1\n",
      "road: 1\n",
      "radical: 2\n",
      "cleric: 1\n",
      "bakri: 5\n",
      "mohammed: 1\n",
      "broadcast: 1\n",
      "flag: 2\n",
      "downing: 1\n",
      "beirut: 1\n",
      "stripped: 1\n",
      "residency: 1\n",
      "rein: 1\n",
      "triggered: 1\n",
      "outrage: 1\n",
      "7: 1\n",
      "never: 1\n",
      "tip: 1\n",
      "knew: 1\n",
      "carry: 1\n",
      "message: 1\n",
      "osama: 1\n",
      "laden: 1\n",
      "community: 2\n",
      "prevent: 1\n",
      "joint: 1\n",
      "mike: 2\n",
      "mullen: 3\n",
      "admiral: 2\n",
      "needed: 1\n",
      "brigadier: 1\n",
      "vahidi: 1\n",
      "violate: 1\n",
      "charter: 1\n",
      "warned: 1\n",
      "drawn: 1\n",
      "defensive: 1\n",
      "regret: 1\n",
      "accuse: 1\n",
      "secretly: 2\n",
      "cover: 1\n",
      "combination: 1\n",
      "diplomatic: 2\n",
      "resolve: 1\n",
      "dispute: 1\n",
      "georgia: 3\n",
      "natural: 3\n",
      "shortage: 1\n",
      "electricity: 2\n",
      "mikhail: 1\n",
      "saakashvili: 2\n",
      "cabinet: 1\n",
      "flowing: 1\n",
      "beginning: 1\n",
      "soviet: 1\n",
      "enduring: 1\n",
      "freezing: 1\n",
      "limited: 1\n",
      "explosion: 3\n",
      "pipeline: 2\n",
      "delivering: 1\n",
      "deny: 1\n",
      "georgian: 2\n",
      "deliberately: 1\n",
      "insist: 1\n",
      "sabotaged: 1\n",
      "repair: 1\n",
      "completed: 1\n",
      "heavily: 1\n",
      "dependent: 1\n",
      "alternative: 1\n",
      "without: 1\n",
      "breakdown: 1\n",
      "unit: 1\n",
      "downed: 1\n",
      "line: 1\n",
      "peru: 3\n",
      "alberto: 1\n",
      "fujimori: 3\n",
      "trying: 1\n",
      "alliance: 1\n",
      "peruvian: 1\n",
      "signature: 3\n",
      "validated: 1\n",
      "joining: 1\n",
      "si: 1\n",
      "cumple: 1\n",
      "notarized: 1\n",
      "content: 1\n",
      "disgraced: 1\n",
      "ex-president: 1\n",
      "son: 1\n",
      "parent: 1\n",
      "citizenship: 1\n",
      "exiled: 1\n",
      "wanted: 1\n",
      "abuse: 1\n",
      "related: 1\n",
      "squad: 1\n",
      "hard-line: 2\n",
      "ruler: 1\n",
      "1990: 1\n",
      "calling: 1\n",
      "politically: 1\n",
      "lima: 1\n",
      "embassy: 4\n",
      "re-opened: 1\n",
      "additional: 1\n",
      "allowed: 1\n",
      "visa: 1\n",
      "thanked: 1\n",
      "broken: 1\n",
      "plot: 1\n",
      "confiscated: 1\n",
      "arm: 1\n",
      "explosive: 1\n",
      "advised: 1\n",
      "travel: 1\n",
      "kenyan: 2\n",
      "demonstrating: 1\n",
      "abating: 1\n",
      "tried: 1\n",
      "danish: 2\n",
      "nairobi: 1\n",
      "shouting: 1\n",
      "anti-denmark: 1\n",
      "burning: 1\n",
      "demonstrated: 1\n",
      "middle: 1\n",
      "east: 1\n",
      "india: 3\n",
      "bangladesh: 1\n",
      "faith: 2\n",
      "legal: 1\n",
      "published: 1\n",
      "depicts: 1\n",
      "wearing: 1\n",
      "turban: 1\n",
      "shaped: 1\n",
      "malaysian: 1\n",
      "abdullah: 1\n",
      "badawi: 1\n",
      "demonizing: 1\n",
      "accept: 1\n",
      "equal: 1\n",
      "coal: 2\n",
      "mining: 1\n",
      "accident: 1\n",
      "northwestern: 1\n",
      "searcher: 1\n",
      "recovered: 1\n",
      "missing: 1\n",
      "miner: 2\n",
      "state-run: 1\n",
      "39: 1\n",
      "working: 1\n",
      "privately-owned: 1\n",
      "mine: 3\n",
      "shaanxi: 1\n",
      "managed: 1\n",
      "reach: 1\n",
      "trapped: 1\n",
      "owner: 1\n",
      "manager: 1\n",
      "provided: 1\n",
      "cave-ins: 1\n",
      "flood: 1\n",
      "crackdown: 1\n",
      "safety: 1\n",
      "focus: 1\n",
      "kashmir: 3\n",
      "confidence-building: 1\n",
      "two-day: 1\n",
      "led: 1\n",
      "chart: 1\n",
      "course: 1\n",
      "future: 1\n",
      "reduction: 1\n",
      "proposal: 1\n",
      "self-rule: 1\n",
      "reveal: 1\n",
      "blast: 1\n",
      "rounded: 1\n",
      "428: 1\n",
      "suburb: 1\n",
      "score: 1\n",
      "al-qaida-linked: 1\n",
      "gunning: 1\n",
      "wael: 1\n",
      "al-rubaei: 1\n",
      "guardian: 1\n",
      "block: 1\n",
      "inspection: 1\n",
      "enrichment: 3\n",
      "referred: 1\n",
      "12-member: 1\n",
      "deciding: 1\n",
      "contradict: 1\n",
      "constitution: 1\n",
      "jeopardize: 1\n",
      "expects: 1\n",
      "collapsed: 1\n",
      "converting: 1\n",
      "precursor: 1\n",
      "allay: 1\n",
      "pursuing: 1\n",
      "zambian: 1\n",
      "levy: 1\n",
      "mwanawasa: 5\n",
      "mulongoti: 1\n",
      "address: 1\n",
      "remains: 1\n",
      "treated: 1\n",
      "hypertension: 1\n",
      "suffering: 1\n",
      "stroke: 2\n",
      "dominated: 1\n",
      "crisis: 1\n",
      "zimbabwe: 1\n",
      "absence: 1\n",
      "zimbabwean: 1\n",
      "robert: 1\n",
      "mugabe: 1\n",
      "pressure: 1\n",
      "harare: 1\n",
      "59: 1\n",
      "current: 1\n",
      "mild: 1\n",
      "named: 2\n",
      "gene: 1\n",
      "sperling: 4\n",
      "oversees: 1\n",
      "administration: 4\n",
      "clinton: 1\n",
      "succeeds: 1\n",
      "lawrence: 1\n",
      "clinton-era: 1\n",
      "spent: 1\n",
      "counselor: 1\n",
      "treasury: 1\n",
      "timothy: 1\n",
      "geithner: 1\n",
      "announcement: 1\n",
      "window: 1\n",
      "manufacturer: 1\n",
      "d.c.: 1\n",
      "appointment: 1\n",
      "planning: 1\n",
      "re-election: 1\n",
      "ex-clinton: 1\n",
      "william: 1\n",
      "daley: 1\n",
      "replacing: 1\n",
      "rahm: 1\n",
      "emmanuel: 1\n",
      "bosnia-herzegovina: 1\n",
      "bosnian: 4\n",
      "serb: 4\n",
      "officer: 1\n",
      "massacre: 2\n",
      "srebrenica: 2\n",
      "37-year-old: 1\n",
      "dragan: 1\n",
      "neskovic: 1\n",
      "northeastern: 1\n",
      "bijeljina: 1\n",
      "suspicion: 1\n",
      "8,000: 1\n",
      "boy: 1\n",
      "supposed: 1\n",
      "ii: 1\n",
      "hague: 1\n",
      "sentenced: 1\n",
      "radovan: 1\n",
      "karadzic: 1\n",
      "established: 1\n",
      "workload: 1\n",
      "hague-based: 1\n",
      "archaeologist: 1\n",
      "discovered: 1\n",
      "ruin: 1\n",
      "29: 1\n",
      "date: 1\n",
      "4,500: 1\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    words = dataset['clean_dataset']\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e39ed02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "features = countvec.fit_transform(dataset['clean_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53ee474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915136476426799\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(dataset['clean_dataset'])\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, dataset['POS'], test_size=0.2)\n",
    "\n",
    "# Classifier\n",
    "nb_model_tfidf = MultinomialNB()\n",
    "nb_model_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_tfidf = nb_model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "print(accuracy_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b376eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thousand</th>\n",
       "      <th>marched</th>\n",
       "      <th>london</th>\n",
       "      <th>protest</th>\n",
       "      <th>war</th>\n",
       "      <th>iraq</th>\n",
       "      <th>demand</th>\n",
       "      <th>british</th>\n",
       "      <th>troop</th>\n",
       "      <th>country</th>\n",
       "      <th>...</th>\n",
       "      <th>sperling</th>\n",
       "      <th>oversees</th>\n",
       "      <th>administration</th>\n",
       "      <th>clinton</th>\n",
       "      <th>window</th>\n",
       "      <th>bosnian</th>\n",
       "      <th>serb</th>\n",
       "      <th>massacre</th>\n",
       "      <th>srebrenica</th>\n",
       "      <th>hague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10070</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10071</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10072</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10073</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10074</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10075 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       thousand  marched  london  protest  war  iraq  demand  british  troop  \\\n",
       "0             0        0       0        0    0     0       0        0      0   \n",
       "1             0        0       0        0    0     0       0        0      0   \n",
       "2             0        0       0        0    0     0       0        0      0   \n",
       "3             0        0       0        0    0     0       0        0      0   \n",
       "4             0        0       0        0    0     0       0        0      0   \n",
       "...         ...      ...     ...      ...  ...   ...     ...      ...    ...   \n",
       "10070         0        0       0        0    0     0       0        0      0   \n",
       "10071         0        0       0        0    0     0       0        0      0   \n",
       "10072         0        0       0        0    0     0       0        0      0   \n",
       "10073         0        0       0        0    0     0       0        0      0   \n",
       "10074         0        0       0        0    0     0       0        0      0   \n",
       "\n",
       "       country  ...  sperling  oversees  administration  clinton  window  \\\n",
       "0            0  ...         0         0               0        0       0   \n",
       "1            0  ...         0         0               0        0       0   \n",
       "2            0  ...         0         0               0        0       0   \n",
       "3            0  ...         0         0               0        0       0   \n",
       "4            0  ...         0         0               0        0       0   \n",
       "...        ...  ...       ...       ...             ...      ...     ...   \n",
       "10070        0  ...         0         0               0        0       0   \n",
       "10071        0  ...         0         0               0        0       0   \n",
       "10072        0  ...         0         0               0        0       0   \n",
       "10073        0  ...         0         0               0        0       0   \n",
       "10074        0  ...         0         0               0        0       0   \n",
       "\n",
       "       bosnian  serb  massacre  srebrenica  hague  \n",
       "0            0     0         0           0      0  \n",
       "1            0     0         0           0      0  \n",
       "2            0     0         0           0      0  \n",
       "3            0     0         0           0      0  \n",
       "4            0     0         0           0      0  \n",
       "...        ...   ...       ...         ...    ...  \n",
       "10070        0     0         0           0      0  \n",
       "10071        0     0         0           0      0  \n",
       "10072        0     0         0           0      0  \n",
       "10073        1     0         0           0      0  \n",
       "10074        0     0         0           0      0  \n",
       "\n",
       "[10075 rows x 1000 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features.toarray(),columns=countvec.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b423842",
   "metadata": {},
   "source": [
    "# ML Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9694daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, dataset['POS'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72c0fdcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8060, 1000)\n",
      "(2015, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d09391bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "#training\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3af5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913151364764268\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e2fa084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913151364764268"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04737622",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "892641f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9259305210918114\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training data\n",
    "train_predictions = model.predict(X_train)\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b32262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1755    0]\n",
      " [ 175   85]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cd7744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdcca019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      1755\n",
      "           1       1.00      0.33      0.49       260\n",
      "\n",
      "    accuracy                           0.91      2015\n",
      "   macro avg       0.95      0.66      0.72      2015\n",
      "weighted avg       0.92      0.91      0.89      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa9a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
